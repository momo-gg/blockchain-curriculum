# 3.2 How Solana Works

---

## Why This Matters

Understanding Solana's internals helps you:
- Debug issues in your applications
- Choose the right infrastructure
- Understand trade-offs in design decisions
- Appreciate why certain behaviors exist

This module covers the eight innovations in depth, plus the network architecture and client ecosystem.

---

## ðŸŸ¢ General Understanding

### The Eight Innovations Working Together

Think of Solana as a factory assembly line:

```mermaid
flowchart TD
    A[Transaction arrives] --> B[Leader receives]
    B --> C[Transaction ordered]
    C --> D[Transaction executed]
    D --> E[Block created]
    E --> F[Block propagated]
    F --> G[Block confirmed]
    G --> H[State updated]

    style B stroke:#FFD700,stroke-width:3px
    style D stroke:#FFD700,stroke-width:3px
    style G stroke:#FFD700,stroke-width:3px

    B -.Gulf Stream: skip the waiting room.-> B
    C -.PoH: stamp with verifiable time.-> C
    D -.Sealevel: process in parallel.-> D
    E -.Pipelining: like a CPU pipeline.-> E
    F -.Turbine: spread like gossip.-> F
    G -.Tower BFT: validators vote.-> G
    H -.Cloudbreak: save to disk fast.-> H
```

Each innovation handles one step, optimized for speed.

### Example: One Transaction End-to-End

```mermaid
flowchart LR
    A[User clicks Swap] --> B[Wallet signs transaction]
    B --> C[RPC forwards to current leader]
    C --> D[Leader executes in Sealevel]
    D --> E[Block propagates via Turbine]
    E --> F[Validators vote Tower BFT]
    F --> G[Confirmation returned to client]

    style D stroke:#FFD700,stroke-width:3px,color:#FFD700
    style F stroke:#FFD700,stroke-width:3px,color:#FFD700
```

This is why apps can feel fast: the system is built for short, predictable
paths from intent to confirmation.

### How the Peer-to-Peer Network Stays Connected

**The Gossip Protocol**

How do thousands of computers find and talk to each other without a central server?

```mermaid
flowchart TD
    subgraph Traditional["Traditional (Client-Server)"]
        C1[Computer 1] --> CS[Central Server]
        C2[Computer 2] --> CS
        C3[Computer 3] --> CS
        CS --> C1
        CS --> C2
        CS --> C3
    end

    subgraph P2P["Peer-to-Peer (Gossip)"]
        P1[Computer A] --> P2[Computer B]
        P2 --> P3[Computer C]
        P1 --> P3
        P3 --> P4[Computer D]
        P2 --> P4
    end

    style CS stroke:#FF0000,stroke-width:3px
    style P2P stroke:#FFD700,stroke-width:3px
```

**How Gossip Works**

```mermaid
flowchart TD
    A[Node A<br/>learns something new] --> B[Node B]
    A --> C[Node C]
    A --> D[Node D]
    A --> E[Node E]

    B --> H[Node H]
    B --> I[Node I]
    C --> K[Node K]
    C --> L[Node L]

    style A stroke:#FFD700,stroke-width:3px,color:#FFD700
```

**Why Random Peer Selection?**

- If peers were fixed, attackers could target them
- Random selection means unpredictable topology
- Network self-heals (if peer dies, choose another)

**Seed Nodes**

New validators need to find the network:

```mermaid
flowchart LR
    A[New validator starts] --> B[Contacts seed nodes]
    B --> C[Receives list of active peers]
    C --> D[Connects to peers]
    D --> E[Learns about more peers via gossip]
    E --> F[Has many connections]

    style B stroke:#FFD700,stroke-width:3px,color:#FFD700
    style E stroke:#FFD700,stroke-width:3px,color:#FFD700
```

Seed nodes are bootstrap points, not central authorities. After connecting, they're not needed.

### How Solana Launched from Genesis

Every blockchain starts from a "genesis block." But how?

**Genesis Configuration**

The genesis defines initial state:
```yaml
# Illustrative (not actual genesis values)
cluster_type: mainnet-beta
genesis_time: "<timestamp>"

initial_validators:
  - pubkey: "<validator_pubkey>"
    stake: <lamports>

initial_accounts:
  - address: "<account_pubkey>"
    lamports: <lamports>

programs:
  - system_program: "11111111..."
```

**The Launch Sequence**

```mermaid
flowchart TD
    A[PREPARATION] --> B[GENESIS T=0]
    B --> C[FIRST BLOCKS]
    C --> D[OPENING UP]

    A1[Genesis configuration finalized] --> A
    A2[Founding validators set up hardware] --> A
    A3[Testnet runs] --> A

    B1[All validators start simultaneously] --> B
    B2[Load identical genesis configuration] --> B
    B3[First PoH tick begins] --> B

    C1[Leaders selected from genesis validator set] --> C
    C2[First transactions processed] --> C

    D1[New validators can join] --> D
    D2[Stake delegation enabled] --> D

    style B stroke:#FFD700,stroke-width:3px,color:#FFD700
```

**Why This Works**

All validators:
- Start with identical state (genesis)
- Follow identical rules (client software)
- Build on the same chain

The chain "emerges" from consensus, not from a central server.

> âœ… **Check Your Understanding**
> - [ ] How does gossip protocol spread information?
> - [ ] What are seed nodes and why are they needed?
> - [ ] What's in a genesis configuration?
> - [ ] How do all validators start with the same state?

---

## ðŸŸ¡ PM/EM Depth

### What Are Blockchain "Clients"?

A **client** is software that implements the blockchain protocol. Think of it like different web browsers implementing HTTP:

```mermaid
flowchart LR
    subgraph Protocol["Protocol (Rules)"]
        SP[Solana Protocol]
        HP[HTTP Protocol]
    end

    subgraph Implementations["Clients (Implementations)"]
        A[Agave<br/>Rust, by Anza]
        F[Firedancer<br/>C, by Jump]
        S[Sig<br/>Zig, by Syndica]
    end

    subgraph Browsers["Browser Implementations"]
        CH[Chrome]
        FF[Firefox]
        SF[Safari]
    end

    SP --> A
    SP --> F
    SP --> S

    HP --> CH
    HP --> FF
    HP --> SF

    style A stroke:#FFD700,stroke-width:3px
```

**Why Multiple Clients Matter**

| Benefit | Explanation |
|---------|-------------|
| No single point of failure | Bug in one client doesn't affect others |
| Defense in depth | Different codebases, different bugs |
| Competition | Drives performance improvements |
| Decentralization | No single team controls the software |

**Solana's Client History**

```mermaid
flowchart TD
    O[Original Solana Labs client] --> A[Agave by Anza<br/>mainnet client]
    O --> F[Firedancer by Jump Crypto<br/>new client in C]
    O --> S[Sig by Syndica<br/>Zig implementation]

    style A stroke:#FFD700,stroke-width:3px,color:#FFD700
```

**Anza and Agave**

Anza maintains the Agave validator client (as of [docs.anza.xyz](https://docs.anza.xyz/)).

### Why Was the Solana Labs GitHub Archived?

The `solana-labs/solana` repository is archived on GitHub, while the active validator client is developed in the `anza-xyz/agave` repository (as of the GitHub repository status and [docs.anza.xyz](https://docs.anza.xyz/)).

### Firedancer: Why It Matters

Jump Crypto is building Firedancer, a new Solana validator client written in C (as of [Jump Crypto](https://jumpcrypto.com/firedancer)):

**Technical Goal**
- C-based implementation focused on performance

**What It Means for the Network**

```mermaid
flowchart TD
    subgraph Today
        A[Agave<br/>primary mainnet client]
        F[Firedancer<br/>under active development]
    end

    subgraph Future["Future: Multiple Production Clients"]
        AB[Agave has a bug] --> FR[Firedancer validators keep running]
        FB[Firedancer has a bug] --> AR[Agave validators keep running]
        FR --> N[Network stays live]
        AR --> N
    end

    style N stroke:#FFD700,stroke-width:3px,color:#FFD700
```

### Multiple Concurrent Leaders?

Solana expects **one leader per slot**. Blocks from any other validator for that slot are rejected. There can be temporary forks during partitions, but each slot still has a single expected leader according to the leader schedule (as of [Solana leader rotation docs](https://docs.solanalabs.com/consensus/leader-rotation)).

### Alpenglow (Proposed Consensus Rewrite)

**Alpenglow** is a proposed new consensus protocol by Anza that replaces TowerBFT and Proof of History with new components (Votor for voting/finality and Rotor for data dissemination). It is **not deployed** as of today; published targets are based on simulations and research (as of [Anzaâ€™s Alpenglow announcement](https://www.anza.xyz/blog/alpenglow-a-new-consensus-for-solana)). Treat performance numbers as aspirational until implemented and activated.

### Feature Gates: How Protocol Changes Deploy

Unlike traditional software, protocol changes require coordinated validator upgrades and feature activation.

**The Feature Gate System**

```mermaid
flowchart TD
    A[DEVELOPMENT] --> B[ACTIVATION PROPOSAL]
    B --> C[ACTIVATION]
    C --> D[ENFORCEMENT]

    A1[Feature coded, tested, audited] --> A
    A2[Merged with feature flag disabled] --> A

    B1[SIMD written] --> B
    B2[Community discussion] --> B
    B3[Validators signal readiness] --> B

    C1[Feature flag enabled after sufficient stake upgrades] --> C
    C2[Activation occurs at specific slot] --> C

    D1[Blocks valid before activation slot] --> D
    D2[Blocks invalid after activation slot] --> D

    style C stroke:#FFD700,stroke-width:3px,color:#FFD700
```

> ðŸ“Š **Business Context**: Feature gates enable upgrades without hard forks but require validator coordination and social consensus.

### Governance: How Decisions Are Made

Solana governance is primarily off-chain, with protocol changes flowing through SIMDs and validator upgrades (as of [docs.solanalabs.com/proposals](https://docs.solanalabs.com/proposals)):

**Decision-Making Layers**

| Layer | What It Decides | Who Decides |
|-------|-----------------|-------------|
| Protocol | Technical changes (SIMDs) | Core contributors + validators |
| Validator | Run features, choose clients | Individual operators |
| Foundation | Grants, ecosystem support | Foundation board |
| Labs/Anza | Client development priorities | Engineering teams |

**Solana Improvement Documents (SIMDs)**

SIMDs define proposed protocol changes and their discussion process (as of [docs.solanalabs.com/proposals](https://docs.solanalabs.com/proposals)).

**Validator Voting**

On-chain stake-weighted voting for feature activation:
- Validators signal readiness by running upgraded software
- Activation requires a supermajority of stake to upgrade; thresholds are network-configured and can change over time (as of Solana validator documentation)

> âœ… **Check Your Understanding**
> - [ ] What is a blockchain client?
> - [ ] What does Anza maintain today?
> - [ ] What's Firedancer and who's building it?
> - [ ] How do feature gates prevent network splits?
> - [ ] Who decides on protocol changes?

---

## ðŸ”µ Engineer Depth

### Proof of History: Deep Technical Dive

**The Hash Chain**

```rust
// Simplified PoH loop
fn poh_tick(state: &mut PohState) {
    loop {
        state.hash = sha256(&state.hash);
        state.tick_count += 1;

        if state.tick_count % TICKS_PER_SLOT == 0 {
            // Slot boundary
            break;
        }
    }
}
```

**Mixing Data Into PoH**

When transactions arrive:

```rust
fn mix_transaction(state: &mut PohState, tx: &Transaction) {
    // Hash includes previous hash AND transaction data
    let mut hasher = Sha256::new();
    hasher.update(&state.hash);
    hasher.update(&tx.serialize());
    state.hash = hasher.finalize();

    // Record the position
    state.entries.push(Entry {
        hash: state.hash,
        transactions: vec![tx.clone()],
        tick_count: state.tick_count,
    });
}
```

**Verification**

```rust
fn verify_poh(entries: &[Entry]) -> bool {
    let mut expected_hash = entries[0].previous_hash;

    for entry in entries {
        // Verify each entry's hash chain
        for _ in 0..entry.num_ticks {
            expected_hash = sha256(&expected_hash);
        }

        // Mix in transactions
        for tx in &entry.transactions {
            expected_hash = sha256([expected_hash, tx.hash()]);
        }

        if expected_hash != entry.hash {
            return false;
        }
    }
    true
}
```

### Tower BFT Consensus

Tower BFT is PBFT modified to use PoH:

**Vote Lockouts**

When a validator votes for a slot, they're "locked" to that fork:

```
Vote for slot 100:
- Locked for 2 slots (can't vote for conflicting fork for slots 100-101)

Vote for slot 101 (same fork):
- Previous lockout doubles: 4 slots
- New vote: 2 slots

Vote for slot 102 (same fork):
- Previous lockouts: 8 slots, 4 slots
- New vote: 2 slots

Each consecutive vote doubles the oldest lockout.
```

**Why Lockouts Work**

```
Validator wants to vote for conflicting fork:
- Must wait out all lockouts first
- Exponential lockouts mean long waits
- Other validators have moved on
- Fork becomes impossible to finalize

This provides "finality" without explicit finalization messages.
```

> ðŸ’¡ **ELI5: Tower BFT**
>
> Imagine a **betting game where you can change your mind, but it gets expensive**:
> - You bet $1 on Team A. To change your bet, wait 2 minutes.
> - You bet again on Team A. Now wait 4 minutes to change.
> - Bet again? Wait 8 minutes. Then 16. Then 32...
>
> **The clever part**: After a few bets on the same team, you'd have to wait hours to switch. By then, the game is over! Everyone knows you're committed.
>
> **How this creates finality**: Once enough validators have voted on the same chain several times, they're all "locked in" â€” switching would take so long that the network moves on without them. No need to wait for a special "this is final!" message.

**Vote Structure**

```rust
struct Vote {
    slots: Vec<Slot>,           // Slots being voted for
    hash: Hash,                  // Blockhash of last slot
    timestamp: Option<i64>,      // Unix timestamp (leader only)
    authorized_voter: Pubkey,    // Who's voting
}
```

### Turbine: Block Propagation

How do you send large blocks to all validators quickly? (as of [Solana Turbine docs](https://docs.solanalabs.com/consensus/turbine-block-propagation))

**Shredding**

```mermaid
flowchart TD
    A[Block] --> B[Split into shreds]
    B --> C[Add erasure coding]
    C --> D[Reconstruct from subset of shreds]

    style C stroke:#FFD700,stroke-width:3px,color:#FFD700
```

**Tree Propagation**

Instead of leader sending to everyone:

```mermaid
flowchart TD
    L[Leader] --> V1[Validator 1]
    L --> V2[Validator 2]
    L --> V3[Validator 3]

    V1 --> V4[Validator 4]
    V1 --> V5[Validator 5]
    V2 --> V6[Validator 6]
    V2 --> V7[Validator 7]
    V3 --> V8[Validator 8]
    V3 --> V9[Validator 9]

    style L stroke:#FFD700,stroke-width:3px,color:#FFD700
```

**Neighborhood Structure**

Validators are organized into layers with stake-weighted selection:

```
High stake: Early layers (more responsibility)
Low stake: Later layers (less critical path)

Benefits:
- High-stake validators are well-connected
- Attack on small validators doesn't affect propagation
- Stake-weighted importance
```

### Gulf Stream: Transaction Forwarding

Traditional blockchains typically expose a public mempool. Solana avoids a public, gossip-based mempool, but validators still maintain local queues and forward transactions directly to scheduled leaders.

**Traditional Mempool**

```mermaid
flowchart LR
    U[User] --> R[RPC]
    R --> M[Mempool<br/>waiting room]
    M --> L[Leader picks transactions]

    style M stroke:#FF0000,stroke-width:3px
```

**Gulf Stream**

```mermaid
flowchart LR
    U[User] --> R[RPC]
    R --> L[Forward directly to<br/>current/next leader]

    style L stroke:#FFD700,stroke-width:3px,color:#FFD700
```

**Leader Schedule**

```
The schedule is known in advance:

Slot 100: Leader = Validator A
Slot 101: Leader = Validator B
Slot 102: Leader = Validator C
Slot 103: Leader = Validator A
...

RPC knows who to send transactions to.
```

**Benefits**
- Lower latency (no mempool queueing)
- Less spam (leaders can apply stake-weighted QoS)
- More predictable (know when your transaction will be processed)



### Sealevel: Parallel Execution

Solana can execute non-conflicting transactions in parallel:

**The Key Insight**

Transactions declare which accounts they'll read/write:

```rust
Transaction {
    accounts: [
        AccountMeta { pubkey: A, is_writable: true },   // Write A
        AccountMeta { pubkey: B, is_writable: false },  // Read B
        AccountMeta { pubkey: C, is_writable: true },   // Write C
    ],
    instructions: [...],
}
```

**Conflict Detection**

```
Transaction 1: Write A, Read B
Transaction 2: Read A, Write C
Transaction 3: Write A, Read D

Conflicts:
- Tx1 and Tx2: Tx1 writes A, Tx2 reads A â†’ CONFLICT
- Tx1 and Tx3: Both write A â†’ CONFLICT
- Tx2 and Tx3: Tx2 reads A, Tx3 writes A â†’ CONFLICT

Execution:
- Tx1 runs first (or Tx3)
- Tx2 runs after (needs Tx1's A)
- Tx3 runs after Tx2 (or before Tx1)

If Transaction 4: Write E, Read F â†’ NO CONFLICT with any, run in parallel
```

**Parallel Execution Engine**

```mermaid
flowchart TD
    A[Batch of Transactions] --> B[Conflict Analysis<br/>Build graph: transaction â†’ accounts accessed]
    B --> C[Parallel Scheduling<br/>Non-conflicting batches run on different CPU cores]
    C --> D1[Core 1<br/>Batch A]
    C --> D2[Core 2<br/>Batch B]
    C --> D3[Core 3<br/>Batch C]
    C --> D4[Core 4<br/>Batch D]

    style C stroke:#FFD700,stroke-width:3px,color:#FFD700
```

### Pipelining: GPU + CPU Coordination

Like a CPU pipeline, different stages happen simultaneously:

```
Time  â†’  T1      T2      T3      T4      T5
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Fetch     B1      B2      B3      B4      B5
          â”‚       â”‚       â”‚       â”‚
Verify    â”‚       B1      B2      B3      B4
          â”‚       â”‚       â”‚       â”‚
Execute   â”‚       â”‚       B1      B2      B3
          â”‚       â”‚       â”‚       â”‚
Commit    â”‚       â”‚       â”‚       B1      B2

B1 = Batch 1, etc.

Different batches at different stages simultaneously.
```

**Hardware Utilization**

| Stage | Hardware |
|-------|----------|
| Fetch | Network card, memory |
| Verify signatures | GPU (parallel crypto) |
| Execute transactions | CPU |
| Commit state | NVMe drive |

### Cloudbreak: Accounts Database

Solana's accounts are stored in a memory-mapped structure:

```
Accounts Database
â”œâ”€â”€ accounts.index (in-memory, maps pubkey â†’ location)
â”œâ”€â”€ accounts/
â”‚   â”œâ”€â”€ slot_100.accounts (mmap'd files)
â”‚   â”œâ”€â”€ slot_101.accounts
â”‚   â””â”€â”€ ...
â””â”€â”€ snapshots/
    â””â”€â”€ slot_X/ (periodic full state)
```

**Memory Mapping**

```c
// Simplified
void* accounts = mmap(NULL, size, PROT_READ | PROT_WRITE,
                      MAP_SHARED, fd, 0);

// Access account like regular memory
Account* acc = &accounts[offset];

// OS handles paging to/from disk
// Fast for sequential access patterns
```

**Why NVMe Matters**

Solana's storage workload benefits from high IOPS and low latency storage; NVMe is recommended in validator requirements (as of [docs.anza.xyz/operations/requirements](https://docs.anza.xyz/operations/requirements)).

> âœ… **Check Your Understanding**
> - [ ] How does PoH provide ordering guarantees?
> - [ ] Explain the vote lockout mechanism in Tower BFT
> - [ ] How does Turbine achieve fast block propagation?
> - [ ] What makes Gulf Stream different from a mempool?
> - [ ] How does Sealevel determine which transactions can run in parallel?

---

## Key Takeaways

1. **PoH is a verifiable clock** â€” proves time without synchronization
2. **Tower BFT uses exponential lockouts** â€” provides finality without explicit messages
3. **Turbine uses tree propagation** â€” O(log n) latency for block spread
4. **Gulf Stream skips mempools** â€” transactions go directly to leaders
5. **Sealevel parallelizes execution** â€” non-conflicting transactions run simultaneously
6. **Multiple clients improve resilience** â€” Agave in production; Firedancer in development (as of [docs.anza.xyz](https://docs.anza.xyz/) and [jumpcrypto.com/firedancer](https://jumpcrypto.com/firedancer))
7. **Gossip keeps the network connected** â€” decentralized peer discovery
8. **Feature gates enable upgrades** â€” coordinated without hard forks

---

## Next

Now let's understand Solana's unique account model â€” the foundation for all program development.

â†’ [3.3 The Solana Account Model](./3.3-solana-account-model.md)
